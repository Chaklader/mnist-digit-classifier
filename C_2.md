# C-2: Advanced Training Methodologies and Optimization

1. Activation and Output Functions
   - Types of Activation Functions
   - Sigmoid, ReLU, and Tanh Comparisons
   - Softmax for Multi-class Classification
   - Matching Activations to Problem Types
2. Training Neural Networks
   - Dataset Division Strategies
   - Overfitting vs Underfitting
   - Early Stopping Techniques
   - Evaluating Model Performance
3. Regularization and Optimization Techniques
   - L1 vs L2 Regularization
   - Dropout Implementation
   - Vanishing and Exploding Gradients
   - Learning Rate Decay and Momentum
4. Advanced Training Approaches
   - Batch vs Stochastic Gradient Descent
   - Mini-batch Processing
   - Optimizer Comparison (SGD, Adam, RMSprop)
   - Random Restart and Local Minima Solutions
