{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Backward Pass\n",
    "\n",
    "\n",
    "##### Understanding Neural Network Backpropagation Flow\n",
    "\n",
    "The backpropagation algorithm is essential for training neural networks. Let me create a Mermaid diagram that illustrates the flow of information in both the forward and backward passes of the provided neural network code:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    x[\"Input Layer<br><br>x = [0.5, 0.1, -0.2]\"] --> hidden_layer_in[\"Hidden Layer Input<br><br>hidden_layer_in = np.dot(x, weights_input_to_hidden)\"]\n",
    "    hidden_layer_in --> hidden_layer_out[\"Hidden Layer Output<br><br>hidden_layer_out = sigmoid(hidden_layer_in)\"]\n",
    "    hidden_layer_out --> output_layer_in[\"Output Layer Input<br><br>output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\"]\n",
    "    output_layer_in --> output_layer_out[\"Output Layer Output<br><br>output_layer_out = sigmoid(output_layer_in)\"]\n",
    "    \n",
    "    target[\"Target Value<br><br>target = 0.6\"] --> error[\"Error Calculation<br><br>error = target - output_layer_out\"]\n",
    "    error --> output_error_term[\"Output Error Term<br><br>output_error_term = error * output_layer_out * (1-output_layer_out)\"]\n",
    "    output_error_term --> hidden_error[\"Hidden Layer Error<br><br>hidden_error = np.dot(output_error_term, weights_hidden_to_output)\"]\n",
    "    hidden_error --> hidden_error_term[\"Hidden Error Term<br><br>hidden_error_term = hidden_error * hidden_layer_out * (1-hidden_layer_out)\"]\n",
    "    \n",
    "    output_error_term --> delta_w_h_o[\"Weight Change (Hidden→Output)<br><br>delta_w_h_o = learnrate * output_error_term * hidden_layer_out\"]\n",
    "    hidden_error_term --> delta_w_i_h[\"Weight Change (Input→Hidden)<br><br>delta_w_i_h = learnrate * np.outer(x, hidden_error_term)\"]\n",
    "    \n",
    "    style x fill:#BCFB89\n",
    "    style target fill:#BCFB89\n",
    "    style hidden_layer_in fill:#9AE4F5\n",
    "    style hidden_layer_out fill:#9AE4F5\n",
    "    style output_layer_in fill:#FBF266\n",
    "    style output_layer_out fill:#FBF266\n",
    "    style error fill:#FA756A\n",
    "    style output_error_term fill:#FA756A\n",
    "    style hidden_error fill:#0096D9\n",
    "    style hidden_error_term fill:#0096D9\n",
    "    style delta_w_h_o fill:#FCEB14\n",
    "    style delta_w_i_h fill:#FCEB14\n",
    "```\n",
    "\n",
    "##### Brief Explanation:\n",
    "\n",
    "The diagram illustrates the complete neural network process in two main phases:\n",
    "\n",
    "**Forward Pass:**\n",
    "1. Input values flow through weighted connections to the hidden layer\n",
    "2. The hidden layer applies the sigmoid activation function\n",
    "3. Hidden layer outputs flow through weighted connections to the output layer\n",
    "4. The output layer applies sigmoid activation to produce the final prediction\n",
    "\n",
    "**Backward Pass:**\n",
    "1. Error is calculated as the difference between target and prediction\n",
    "2. This error propagates backward, first to calculate the output error term\n",
    "3. The error then flows to the hidden layer, accounting for connection weights\n",
    "4. Each layer's error term incorporates the derivative of its activation function\n",
    "5. Weight adjustments are calculated based on these error terms, the learning rate, and the inputs to each connection\n",
    "\n",
    "The diagram color-codes related operations and shows how information flows through the network in both directions. The forward pass learns to make predictions, while the backward pass learns from mistakes to improve the network's parameters.\n",
    "\n",
    "\n",
    "\n",
    "In the neural network code we've been discussing, `delta_w_h_o` and `delta_w_i_h` are the calculated weight adjustments that will be applied to the network's weights during training. These terms are fundamental to how neural networks learn through backpropagation. Let me explain them in detail:\n",
    "\n",
    "`delta_w_h_o` (delta weights hidden to output) represents the changes that should be applied to the weights connecting the hidden layer to the output layer. In our code, it's calculated as:\n",
    "\n",
    "```python\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_out\n",
    "```\n",
    "\n",
    "This calculation combines three factors:\n",
    "1. The learning rate (`learnrate`), which controls how large each adjustment step should be\n",
    "2. The output error term (`output_error_term`), which indicates how much and in which direction the output needs to change\n",
    "3. The hidden layer activations (`hidden_layer_out`), which determine how strongly each hidden neuron contributed to the output\n",
    "\n",
    "The result is a vector showing how much each weight from the hidden layer to the output layer should change. In your specific case, the values were [0.00804047, 0.00555918], meaning the first weight should increase by about 0.008 and the second by about 0.0056.\n",
    "\n",
    "`delta_w_i_h` (delta weights input to hidden) represents the changes for weights connecting the input layer to the hidden layer. It's calculated as:\n",
    "\n",
    "```python\n",
    "delta_w_i_h = learnrate * np.outer(x, hidden_error_term)\n",
    "```\n",
    "\n",
    "This uses the outer product to create a matrix where each element (i,j) tells us how much to adjust the weight from input i to hidden neuron j. The calculation considers:\n",
    "1. The learning rate (`learnrate`)\n",
    "2. The input values (`x`), because connections from more active inputs should be adjusted more\n",
    "3. The hidden layer error terms (`hidden_error_term`), which indicate how each hidden neuron contributed to the overall error\n",
    "\n",
    "Your specific values were:\n",
    "```\n",
    "[[ 1.77005547e-04, -5.11178506e-04]\n",
    " [ 3.54011093e-05, -1.02235701e-04]\n",
    " [-7.08022187e-05,  2.04471402e-04]]\n",
    "```\n",
    "\n",
    "These matrices of weight adjustments are the mechanism by which neural networks learn. After calculating them, they would be added to the current weights:\n",
    "\n",
    "```python\n",
    "weights_hidden_to_output += delta_w_h_o\n",
    "weights_input_to_hidden += delta_w_i_h\n",
    "```\n",
    "\n",
    "Through many iterations of forward passes (making predictions) and backward passes (calculating and applying weight adjustments), the network gradually improves its weights to minimize prediction errors. These small, incremental adjustments are the essence of how neural networks learn from data.\n",
    "\n",
    "The function `np.outer()` in NumPy performs an outer product of two vectors. Let me explain what this means and why it's particularly useful in the neural network's backpropagation algorithm.\n",
    "\n",
    "An outer product takes two vectors and produces a matrix where each element is the product of corresponding elements from the input vectors. If you have a vector of length m and another of length n, their outer product will be a matrix of shape m×n.\n",
    "\n",
    "Mathematically, if we have vectors a and b:\n",
    "- a = [a₁, a₂, ..., aₘ]\n",
    "- b = [b₁, b₂, ..., bₙ]\n",
    "\n",
    "Their outer product a⊗b is a matrix C where each element Cᵢⱼ = aᵢ × bⱼ:\n",
    "\n",
    "```\n",
    "    ⎡a₁b₁  a₁b₂  ...  a₁bₙ⎤\n",
    "    ⎢a₂b₁  a₂b₂  ...  a₂bₙ⎥\n",
    "C = ⎢ .     .    ...   .  ⎥\n",
    "    ⎢ .     .    ...   .  ⎥\n",
    "    ⎣aₘb₁  aₘb₂  ...  aₘbₙ⎦\n",
    "```\n",
    "\n",
    "In our neural network code, we're using:\n",
    "```python\n",
    "delta_w_i_h = learnrate * np.outer(x, hidden_error_term)\n",
    "```\n",
    "\n",
    "Let's break down why this is precisely what we need:\n",
    "\n",
    "1. `x` is our input vector with 3 elements [0.5, 0.1, -0.2]\n",
    "2. `hidden_error_term` is a vector with 2 elements (one for each hidden neuron)\n",
    "3. We need to update a weight matrix `weights_input_to_hidden` that has shape 3×2 (3 input neurons connecting to 2 hidden neurons)\n",
    "\n",
    "The outer product gives us exactly the right shape for our weight updates. Each element (i,j) in the resulting matrix tells us how much to adjust the weight connecting input i to hidden neuron j.\n",
    "\n",
    "To make this concrete, let's work through a simple example. If:\n",
    "- `x` = [0.5, 0.1, -0.2]\n",
    "- `hidden_error_term` = [0.001, -0.002]\n",
    "- `learnrate` = 0.5\n",
    "\n",
    "Then `np.outer(x, hidden_error_term)` would produce:\n",
    "```\n",
    "[0.5 * 0.001    0.5 * (-0.002)]   [0.0005   -0.001 ]\n",
    "[0.1 * 0.001    0.1 * (-0.002)] = [0.0001   -0.0002]\n",
    "[-0.2 * 0.001  -0.2 * (-0.002)]   [-0.0002   0.0004]\n",
    "```\n",
    "\n",
    "After multiplying by the learning rate of 0.5, we get:\n",
    "```\n",
    "[0.00025  -0.0005 ]\n",
    "[0.00005  -0.0001 ]\n",
    "[-0.0001    0.0002]\n",
    "```\n",
    "\n",
    "This matrix has the exact shape needed to update our weight matrix through addition.\n",
    "\n",
    "The beauty of using the outer product here is that it naturally:\n",
    "1. Scales the updates based on both input activations and error terms\n",
    "2. Creates weight updates with the correct shape for our network\n",
    "3. Implements the chain rule from calculus efficiently\n",
    "\n",
    "Without the outer product, we would need to manually iterate through each weight connection and calculate its update individually, which would be less efficient and more prone to errors.\n",
    "\n",
    "In essence, `np.outer()` is creating a matrix of all possible combinations of input activations and hidden errors, which is exactly what the backpropagation algorithm requires for updating the weights between the input and hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "edited": false,
    "gradable": true,
    "grader_id": "idovd1yhep",
    "udacity_user_query": ""
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def forward_pass(x, weights_input_to_hidden, weights_hidden_to_output):\n",
    "    \"\"\"\n",
    "    Make a forward pass through the network\n",
    "    \"\"\"\n",
    "    # Calculate the input to the hidden layer.\n",
    "    hidden_layer_in = np.dot(x, weights_input_to_hidden)\n",
    "    # Calculate the hidden layer output.\n",
    "    hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "    # Calculate the input to the output layer.\n",
    "    output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "    # Calculate the output of the network.\n",
    "    output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "    return hidden_layer_out, output_layer_out\n",
    "\n",
    "def backward_pass(x, target, learnrate, hidden_layer_out, \\\n",
    "                  output_layer_out, weights_hidden_to_output):\n",
    "    \"\"\"\n",
    "    Make a backward pass through the network\n",
    "    \"\"\"\n",
    "    # Calculate output error\n",
    "    # Error is the difference between target and actual output\n",
    "    error = target - output_layer_out\n",
    "    \n",
    "    # Calculate error term for output layer\n",
    "    # For sigmoid activation, error term = error * output * (1 - output)\n",
    "    output_error_term = error * output_layer_out * (1 - output_layer_out)\n",
    "\n",
    "    # Calculate error term for hidden layer\n",
    "    # Error for hidden layer is output_error_term * weights_hidden_to_output\n",
    "    # For sigmoid activation, hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "    hidden_error = np.dot(output_error_term, weights_hidden_to_output)\n",
    "    hidden_error_term = hidden_error * hidden_layer_out * (1 - hidden_layer_out)\n",
    "    \n",
    "    # Calculate change in weights for hidden layer to output layer\n",
    "    # delta_w_h_o = learnrate * output_error_term * hidden_layer_out\n",
    "    delta_w_h_o = learnrate * output_error_term * hidden_layer_out\n",
    "    \n",
    "    # Calculate change in weights for input layer to hidden layer\n",
    "    # delta_w_i_h = learnrate * hidden_error_term * input_values\n",
    "    # Need to reshape for proper outer product dimensions\n",
    "    delta_w_i_h = learnrate * np.outer(x, hidden_error_term)\n",
    "    \n",
    "    return delta_w_h_o, delta_w_i_h\n",
    "\n",
    "# Create data to run through the network\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "weights_input_to_hidden = np.array([\n",
    "    [0.5, -0.6],\n",
    "    [0.1, -0.2],\n",
    "    [0.1, 0.7]\n",
    "])\n",
    "weights_hidden_to_output = np.array([0.1, -0.3])\n",
    "\n",
    "# Forward pass\n",
    "hidden_layer_out, output_layer_out = forward_pass(\n",
    "    x, weights_input_to_hidden, weights_hidden_to_output\n",
    ")\n",
    "\n",
    "# Backward pass\n",
    "delta_w_h_o, delta_w_i_h = backward_pass(\n",
    "    x, target, learnrate, hidden_layer_out, output_layer_out, \\\n",
    "    weights_hidden_to_output\n",
    ")\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Network Backpropagation: A Detailed Explanation\n",
    "\n",
    "The backward pass in neural networks, also known as backpropagation, is the heart of how neural networks learn. Let me walk through this implementation step by step, explaining both the mathematical principles and their practical application in the code.\n",
    "\n",
    "##### Overview of Backpropagation\n",
    "\n",
    "Backpropagation is an algorithm that calculates gradients of the loss function with respect to the network's weights. These gradients guide weight updates to minimize prediction errors. The algorithm works by propagating the error backward through the network, using the chain rule from calculus to determine how each weight contributes to the error.\n",
    "\n",
    "##### Step 1: Calculating the Output Error\n",
    "\n",
    "```python\n",
    "error = target - output_layer_out\n",
    "```\n",
    "\n",
    "This first step computes the raw error by subtracting the network's prediction from the target value. This simple difference measures how far off our prediction is from the desired output. In more complex networks with multiple outputs, this would be a vector of errors.\n",
    "\n",
    "##### Step 2: Computing the Output Layer Error Term\n",
    "\n",
    "```python\n",
    "output_error_term = error * output_layer_out * (1 - output_layer_out)\n",
    "```\n",
    "\n",
    "This step calculates the \"error term\" for the output layer, which incorporates both the raw error and the derivative of the activation function. For the sigmoid function, the derivative is: σ(x) * (1 - σ(x)).\n",
    "\n",
    "The multiplication of three terms:\n",
    "- `error`: How far off our prediction is\n",
    "- `output_layer_out * (1 - output_layer_out)`: The derivative of the sigmoid activation\n",
    "\n",
    "This error term represents how much the output node's activation should change to reduce the error, considering the characteristics of the sigmoid function.\n",
    "\n",
    "##### Step 3: Calculating the Hidden Layer Error\n",
    "\n",
    "```python\n",
    "hidden_error = np.dot(output_error_term, weights_hidden_to_output)\n",
    "```\n",
    "\n",
    "This step propagates the error backward to the hidden layer. We multiply the output error term by the weights connecting the hidden layer to the output layer. This determines how much each hidden node contributed to the output error, weighted by the connection strengths between layers.\n",
    "\n",
    "The dot product here is interesting: since `output_error_term` is a scalar and `weights_hidden_to_output` is a vector, this operation distributes the error across the hidden nodes based on their connection weights to the output.\n",
    "\n",
    "##### Step 4: Computing the Hidden Layer Error Term\n",
    "\n",
    "```python\n",
    "hidden_error_term = hidden_error * hidden_layer_out * (1 - hidden_layer_out)\n",
    "```\n",
    "\n",
    "Similar to Step 2, we calculate the error term for the hidden layer by multiplying:\n",
    "- The propagated error from the output layer (`hidden_error`)\n",
    "- The derivative of the activation function for the hidden layer outputs\n",
    "\n",
    "This error term represents how much each hidden node's activation should change to reduce the overall network error.\n",
    "\n",
    "##### Step 5: Calculating Weight Updates for Hidden-to-Output Connections\n",
    "\n",
    "```python\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_out\n",
    "```\n",
    "\n",
    "Now we calculate how much to adjust the weights between the hidden and output layers. Each weight adjustment is:\n",
    "- Proportional to the learning rate (`learnrate`)\n",
    "- Proportional to the output error term (`output_error_term`)\n",
    "- Proportional to the activation of the hidden node (`hidden_layer_out`)\n",
    "\n",
    "This encapsulates the principle that weights should be adjusted more when:\n",
    "- The error is large\n",
    "- The learning rate is high\n",
    "- The input to that connection (the hidden node activation) is strong\n",
    "\n",
    "##### Step 6: Calculating Weight Updates for Input-to-Hidden Connections\n",
    "\n",
    "```python\n",
    "delta_w_i_h = learnrate * np.outer(x, hidden_error_term)\n",
    "```\n",
    "\n",
    "Finally, we calculate the weight adjustments for connections between the input and hidden layers. The outer product creates a matrix where each element (i,j) represents how much to adjust the weight connecting input node i to hidden node j.\n",
    "\n",
    "The adjustment for each weight is:\n",
    "- Proportional to the learning rate\n",
    "- Proportional to the corresponding hidden node's error term\n",
    "- Proportional to the activation of the input node\n",
    "\n",
    "##### Mathematical Foundation\n",
    "\n",
    "The entire backpropagation process is derived from the chain rule in calculus. For a weight w, we want to compute ∂E/∂w (how the error changes with respect to the weight). By applying the chain rule, we can decompose this into:\n",
    "\n",
    "∂E/∂w = ∂E/∂o × ∂o/∂net × ∂net/∂w\n",
    "\n",
    "Where:\n",
    "- E is the error\n",
    "- o is the output of a neuron\n",
    "- net is the weighted sum input to a neuron\n",
    "- w is the weight\n",
    "\n",
    "The algorithm calculates these partial derivatives layer by layer, moving backward through the network.\n",
    "\n",
    "##### Practical Significance\n",
    "\n",
    "This implementation demonstrates several important principles:\n",
    "1. **Local Computation**: Each node only needs information about its direct connections and error term\n",
    "2. **Weight-Error Relationship**: Weights connecting to nodes with larger errors receive larger updates\n",
    "3. **Activity-Dependent Learning**: Connections between more active nodes receive larger updates\n",
    "4. **Supervised Learning**: The entire process depends on having a target value to calculate the initial error\n",
    "\n",
    "The completed backward pass function returns the calculated weight adjustments (`delta_w_h_o` and `delta_w_i_h`), which would then be applied to update the network's weights:\n",
    "\n",
    "```\n",
    "weights_hidden_to_output += delta_w_h_o\n",
    "weights_input_to_hidden += delta_w_i_h\n",
    "```\n",
    "\n",
    "This process would be repeated over many examples, gradually improving the network's performance by minimizing prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding Weight Update Terms in Neural Network Backpropagation\n",
    "\n",
    "In the neural network code you shared, `delta_w_h_o` and `delta_w_i_h` represent the calculated adjustments that should be made to the network's weights during learning. Let me explain what each of these terms means and how they function in the training process.\n",
    "\n",
    "##### What is delta_w_h_o?\n",
    "\n",
    "`delta_w_h_o` stands for \"delta weights hidden to output.\" This variable represents the changes that should be applied to the weights connecting the hidden layer to the output layer.\n",
    "\n",
    "In the code, it's calculated as:\n",
    "```python\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_out\n",
    "```\n",
    "\n",
    "Breaking this down:\n",
    "- `learnrate` (0.5 in your example) controls how big the weight updates should be. A larger learning rate means larger steps in the weight space.\n",
    "- `output_error_term` represents how much the output was wrong, scaled by the derivative of the activation function. This tells us which direction to move in the weight space.\n",
    "- `hidden_layer_out` represents the activations of the hidden layer neurons. This ensures that connections from more active neurons receive proportionally larger updates.\n",
    "\n",
    "The shape of `delta_w_h_o` matches the shape of `weights_hidden_to_output`. In your example, `hidden_layer_out` is a vector with 2 elements (because you have 2 hidden neurons), so `delta_w_h_o` will also be a vector with 2 elements.\n",
    "\n",
    "Each element tells you how much to adjust the corresponding weight from a hidden neuron to the output neuron.\n",
    "\n",
    "##### What is delta_w_i_h?\n",
    "\n",
    "`delta_w_i_h` stands for \"delta weights input to hidden.\" This variable represents the changes that should be applied to the weights connecting the input layer to the hidden layer.\n",
    "\n",
    "In the code, it's calculated as:\n",
    "```python\n",
    "delta_w_i_h = learnrate * np.outer(x, hidden_error_term)\n",
    "```\n",
    "\n",
    "This calculation is more complex because it involves connections between multiple input neurons and multiple hidden neurons. Breaking it down:\n",
    "- `learnrate` again controls the size of the updates.\n",
    "- `x` is the input vector (with values [0.5, 0.1, -0.2] in your example).\n",
    "- `hidden_error_term` represents how much each hidden neuron contributed to the output error, scaled by the derivative of its activation function.\n",
    "- `np.outer()` creates a matrix where each element (i,j) is the product of the ith element of the first array and the jth element of the second array.\n",
    "\n",
    "The shape of `delta_w_i_h` matches the shape of `weights_input_to_hidden`. In your example, this will be a 3×2 matrix because you have 3 input features and 2 hidden neurons.\n",
    "\n",
    "Each element (i,j) in this matrix tells you how much to adjust the weight connecting input neuron i to hidden neuron j.\n",
    "\n",
    "##### How These Deltas Are Used\n",
    "\n",
    "After calculating these delta terms, the actual weight update would typically happen with:\n",
    "```python\n",
    "weights_hidden_to_output += delta_w_h_o\n",
    "weights_input_to_hidden += delta_w_i_h\n",
    "```\n",
    "\n",
    "This is the core of how neural networks learn. By iteratively:\n",
    "1. Making predictions (forward pass)\n",
    "2. Calculating errors\n",
    "3. Computing weight adjustments (backward pass)\n",
    "4. Updating weights\n",
    "\n",
    "The network gradually improves its predictions by adjusting its internal parameters.\n",
    "\n",
    "##### Concrete Example\n",
    "\n",
    "Let's say after running the code, we get:\n",
    "- `delta_w_h_o = [0.01, -0.02]`\n",
    "- `delta_w_i_h = [[0.005, -0.008], [0.001, -0.0016], [-0.002, 0.0032]]`\n",
    "\n",
    "This would mean:\n",
    "- The weight connecting the first hidden neuron to the output should increase by 0.01\n",
    "- The weight connecting the second hidden neuron to the output should decrease by 0.02\n",
    "- The weights connecting inputs to hidden neurons should change according to the `delta_w_i_h` matrix\n",
    "\n",
    "These small adjustments, repeated over many training examples, enable the network to gradually learn patterns in the data and make increasingly accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of the Calculated Weight Adjustments\n",
    "\n",
    "When you ran the backpropagation code, it produced these weight adjustment values:\n",
    "\n",
    "For hidden layer to output layer:\n",
    "```\n",
    "delta_w_h_o = [0.00804047, 0.00555918]\n",
    "```\n",
    "\n",
    "For input layer to hidden layer:\n",
    "```\n",
    "delta_w_i_h = [[ 1.77005547e-04, -5.11178506e-04]\n",
    "               [ 3.54011093e-05, -1.02235701e-04]\n",
    "               [-7.08022187e-05,  2.04471402e-04]]\n",
    "```\n",
    "\n",
    "Let me explain what these specific values tell us about the learning process in your network:\n",
    "\n",
    "##### Hidden to Output Layer Adjustments\n",
    "\n",
    "The `delta_w_h_o` values [0.00804047, 0.00555918] indicate that:\n",
    "\n",
    "1. The weight connecting the first hidden neuron to the output neuron should increase by approximately 0.008.\n",
    "2. The weight connecting the second hidden neuron to the output neuron should increase by approximately 0.0056.\n",
    "\n",
    "Both adjustments are positive, suggesting that strengthening these connections will help reduce the prediction error. The first hidden neuron's connection requires a slightly larger adjustment than the second, indicating it has more influence on correcting the current error.\n",
    "\n",
    "##### Input to Hidden Layer Adjustments\n",
    "\n",
    "The `delta_w_i_h` matrix shows smaller adjustments, mostly in the order of 10^-4 or 10^-5:\n",
    "\n",
    "```\n",
    "[[ 1.77005547e-04, -5.11178506e-04]\n",
    " [ 3.54011093e-05, -1.02235701e-04]\n",
    " [-7.08022187e-05,  2.04471402e-04]]\n",
    "```\n",
    "\n",
    "Reading this matrix row by row:\n",
    "\n",
    "1. For the first input neuron (with value 0.5):\n",
    "   - Its connection to the first hidden neuron should increase by 0.000177\n",
    "   - Its connection to the second hidden neuron should decrease by 0.000511\n",
    "\n",
    "2. For the second input neuron (with value 0.1):\n",
    "   - Its connection to the first hidden neuron should increase by 0.0000354\n",
    "   - Its connection to the second hidden neuron should decrease by 0.000102\n",
    "\n",
    "3. For the third input neuron (with value -0.2):\n",
    "   - Its connection to the first hidden neuron should decrease by 0.0000708\n",
    "   - Its connection to the second hidden neuron should increase by 0.000204\n",
    "\n",
    "The pattern of positive and negative adjustments reveals the complex interplay between inputs and hidden layer activations in correcting the network's prediction.\n",
    "\n",
    "##### Interpretation of the Values\n",
    "\n",
    "These relatively small weight adjustments are typical in neural network training. A few observations:\n",
    "\n",
    "1. The hidden-to-output weight adjustments are larger than the input-to-hidden adjustments. This is common in backpropagation, where gradients often diminish as they propagate backward (known as the \"vanishing gradient problem\" in deeper networks).\n",
    "\n",
    "2. The signs of the adjustments (positive or negative) indicate the direction needed to reduce error. Positive adjustments strengthen connections, while negative ones weaken them.\n",
    "\n",
    "3. The magnitudes reveal which connections are most important to adjust. For example, the largest adjustment is 0.00804047 for the first hidden-to-output weight.\n",
    "\n",
    "If you were to apply these adjustments to the original weights:\n",
    "\n",
    "```python\n",
    "# Original weights\n",
    "weights_hidden_to_output = np.array([0.1, -0.3])\n",
    "weights_input_to_hidden = np.array([\n",
    "    [0.5, -0.6],\n",
    "    [0.1, -0.2],\n",
    "    [0.1, 0.7]\n",
    "])\n",
    "\n",
    "# Updated weights\n",
    "weights_hidden_to_output += delta_w_h_o  # [0.10804047, -0.29444082]\n",
    "weights_input_to_hidden += delta_w_i_h   # Small adjustments to each weight\n",
    "```\n",
    "\n",
    "After many iterations of this process across multiple training examples, these small adjustments would accumulate and help the network converge toward weights that minimize prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the Backward Pass Implementation:\n",
    "\n",
    "1. **Calculate output error**: I computed the error as the difference between the target value and the actual output from our network.\n",
    "\n",
    "2. **Calculate error term for output layer**: For a sigmoid activation function, the error term is calculated as error * output * (1 - output). This formula comes from the derivative of the sigmoid function combined with the chain rule from calculus.\n",
    "\n",
    "3. **Calculate error term for hidden layer**: This is a two-step process:\n",
    "   - First, I propagated the error backward from the output layer to the hidden layer by multiplying the output error term with the weights connecting the hidden layer to the output layer.\n",
    "   - Then, I applied the sigmoid derivative to get the hidden layer error term.\n",
    "\n",
    "4. **Calculate weight changes for hidden-to-output layer**: The weight changes are computed as the learning rate multiplied by the output error term and the hidden layer outputs.\n",
    "\n",
    "5. **Calculate weight changes for input-to-hidden layer**: Here, I used the outer product of the input values and the hidden error terms, scaled by the learning rate. This gives us the appropriate weight update matrix with the same dimensions as the original weight matrix.\n",
    "\n",
    "This implementation follows the standard backpropagation algorithm, which adjusts weights based on the calculated error to minimize the difference between predicted and actual outputs."
   ]
  }
 ],
 "metadata": {
  "grader_mode": "",
  "kernelspec": {
   "display_name": "aipnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "showGradeBtn": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
