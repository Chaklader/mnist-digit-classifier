{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "edited": false,
    "gradable": true,
    "grader_id": "idovd1yhep",
    "udacity_user_query": ""
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def forward_pass(x, weights_input_to_hidden, weights_hidden_to_output):\n",
    "    \"\"\"\n",
    "    Make a forward pass through the network\n",
    "    \"\"\"\n",
    "    # Calculate the input to the hidden layer.\n",
    "    hidden_layer_in = np.dot(x, weights_input_to_hidden)\n",
    "    # Calculate the hidden layer output.\n",
    "    hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "    # Calculate the input to the output layer.\n",
    "    output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "    # Calculate the output of the network.\n",
    "    output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "    return hidden_layer_out, output_layer_out\n",
    "\n",
    "def backward_pass(x, target, learnrate, hidden_layer_out, \\\n",
    "                  output_layer_out, weights_hidden_to_output):\n",
    "    \"\"\"\n",
    "    Make a backward pass through the network\n",
    "    \"\"\"\n",
    "    # Calculate output error\n",
    "    # Error is the difference between target and actual output\n",
    "    error = target - output_layer_out\n",
    "    \n",
    "    # Calculate error term for output layer\n",
    "    # For sigmoid activation, error term = error * output * (1 - output)\n",
    "    output_error_term = error * output_layer_out * (1 - output_layer_out)\n",
    "\n",
    "    # Calculate error term for hidden layer\n",
    "    # Error for hidden layer is output_error_term * weights_hidden_to_output\n",
    "    # For sigmoid activation, hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "    hidden_error = np.dot(output_error_term, weights_hidden_to_output)\n",
    "    hidden_error_term = hidden_error * hidden_layer_out * (1 - hidden_layer_out)\n",
    "    \n",
    "    # Calculate change in weights for hidden layer to output layer\n",
    "    # delta_w_h_o = learnrate * output_error_term * hidden_layer_out\n",
    "    delta_w_h_o = learnrate * output_error_term * hidden_layer_out\n",
    "    \n",
    "    # Calculate change in weights for input layer to hidden layer\n",
    "    # delta_w_i_h = learnrate * hidden_error_term * input_values\n",
    "    # Need to reshape for proper outer product dimensions\n",
    "    delta_w_i_h = learnrate * np.outer(x, hidden_error_term)\n",
    "    \n",
    "    return delta_w_h_o, delta_w_i_h\n",
    "\n",
    "# Create data to run through the network\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "weights_input_to_hidden = np.array([\n",
    "    [0.5, -0.6],\n",
    "    [0.1, -0.2],\n",
    "    [0.1, 0.7]\n",
    "])\n",
    "weights_hidden_to_output = np.array([0.1, -0.3])\n",
    "\n",
    "# Forward pass\n",
    "hidden_layer_out, output_layer_out = forward_pass(\n",
    "    x, weights_input_to_hidden, weights_hidden_to_output\n",
    ")\n",
    "\n",
    "# Backward pass\n",
    "delta_w_h_o, delta_w_i_h = backward_pass(\n",
    "    x, target, learnrate, hidden_layer_out, output_layer_out, \\\n",
    "    weights_hidden_to_output\n",
    ")\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Network Backpropagation: A Detailed Explanation\n",
    "\n",
    "The backward pass in neural networks, also known as backpropagation, is the heart of how neural networks learn. Let me walk through this implementation step by step, explaining both the mathematical principles and their practical application in the code.\n",
    "\n",
    "##### Overview of Backpropagation\n",
    "\n",
    "Backpropagation is an algorithm that calculates gradients of the loss function with respect to the network's weights. These gradients guide weight updates to minimize prediction errors. The algorithm works by propagating the error backward through the network, using the chain rule from calculus to determine how each weight contributes to the error.\n",
    "\n",
    "##### Step 1: Calculating the Output Error\n",
    "\n",
    "```python\n",
    "error = target - output_layer_out\n",
    "```\n",
    "\n",
    "This first step computes the raw error by subtracting the network's prediction from the target value. This simple difference measures how far off our prediction is from the desired output. In more complex networks with multiple outputs, this would be a vector of errors.\n",
    "\n",
    "##### Step 2: Computing the Output Layer Error Term\n",
    "\n",
    "```python\n",
    "output_error_term = error * output_layer_out * (1 - output_layer_out)\n",
    "```\n",
    "\n",
    "This step calculates the \"error term\" for the output layer, which incorporates both the raw error and the derivative of the activation function. For the sigmoid function, the derivative is: σ(x) * (1 - σ(x)).\n",
    "\n",
    "The multiplication of three terms:\n",
    "- `error`: How far off our prediction is\n",
    "- `output_layer_out * (1 - output_layer_out)`: The derivative of the sigmoid activation\n",
    "\n",
    "This error term represents how much the output node's activation should change to reduce the error, considering the characteristics of the sigmoid function.\n",
    "\n",
    "##### Step 3: Calculating the Hidden Layer Error\n",
    "\n",
    "```python\n",
    "hidden_error = np.dot(output_error_term, weights_hidden_to_output)\n",
    "```\n",
    "\n",
    "This step propagates the error backward to the hidden layer. We multiply the output error term by the weights connecting the hidden layer to the output layer. This determines how much each hidden node contributed to the output error, weighted by the connection strengths between layers.\n",
    "\n",
    "The dot product here is interesting: since `output_error_term` is a scalar and `weights_hidden_to_output` is a vector, this operation distributes the error across the hidden nodes based on their connection weights to the output.\n",
    "\n",
    "##### Step 4: Computing the Hidden Layer Error Term\n",
    "\n",
    "```python\n",
    "hidden_error_term = hidden_error * hidden_layer_out * (1 - hidden_layer_out)\n",
    "```\n",
    "\n",
    "Similar to Step 2, we calculate the error term for the hidden layer by multiplying:\n",
    "- The propagated error from the output layer (`hidden_error`)\n",
    "- The derivative of the activation function for the hidden layer outputs\n",
    "\n",
    "This error term represents how much each hidden node's activation should change to reduce the overall network error.\n",
    "\n",
    "##### Step 5: Calculating Weight Updates for Hidden-to-Output Connections\n",
    "\n",
    "```python\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_out\n",
    "```\n",
    "\n",
    "Now we calculate how much to adjust the weights between the hidden and output layers. Each weight adjustment is:\n",
    "- Proportional to the learning rate (`learnrate`)\n",
    "- Proportional to the output error term (`output_error_term`)\n",
    "- Proportional to the activation of the hidden node (`hidden_layer_out`)\n",
    "\n",
    "This encapsulates the principle that weights should be adjusted more when:\n",
    "- The error is large\n",
    "- The learning rate is high\n",
    "- The input to that connection (the hidden node activation) is strong\n",
    "\n",
    "##### Step 6: Calculating Weight Updates for Input-to-Hidden Connections\n",
    "\n",
    "```python\n",
    "delta_w_i_h = learnrate * np.outer(x, hidden_error_term)\n",
    "```\n",
    "\n",
    "Finally, we calculate the weight adjustments for connections between the input and hidden layers. The outer product creates a matrix where each element (i,j) represents how much to adjust the weight connecting input node i to hidden node j.\n",
    "\n",
    "The adjustment for each weight is:\n",
    "- Proportional to the learning rate\n",
    "- Proportional to the corresponding hidden node's error term\n",
    "- Proportional to the activation of the input node\n",
    "\n",
    "##### Mathematical Foundation\n",
    "\n",
    "The entire backpropagation process is derived from the chain rule in calculus. For a weight w, we want to compute ∂E/∂w (how the error changes with respect to the weight). By applying the chain rule, we can decompose this into:\n",
    "\n",
    "∂E/∂w = ∂E/∂o × ∂o/∂net × ∂net/∂w\n",
    "\n",
    "Where:\n",
    "- E is the error\n",
    "- o is the output of a neuron\n",
    "- net is the weighted sum input to a neuron\n",
    "- w is the weight\n",
    "\n",
    "The algorithm calculates these partial derivatives layer by layer, moving backward through the network.\n",
    "\n",
    "##### Practical Significance\n",
    "\n",
    "This implementation demonstrates several important principles:\n",
    "1. **Local Computation**: Each node only needs information about its direct connections and error term\n",
    "2. **Weight-Error Relationship**: Weights connecting to nodes with larger errors receive larger updates\n",
    "3. **Activity-Dependent Learning**: Connections between more active nodes receive larger updates\n",
    "4. **Supervised Learning**: The entire process depends on having a target value to calculate the initial error\n",
    "\n",
    "The completed backward pass function returns the calculated weight adjustments (`delta_w_h_o` and `delta_w_i_h`), which would then be applied to update the network's weights:\n",
    "\n",
    "```\n",
    "weights_hidden_to_output += delta_w_h_o\n",
    "weights_input_to_hidden += delta_w_i_h\n",
    "```\n",
    "\n",
    "This process would be repeated over many examples, gradually improving the network's performance by minimizing prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding Weight Update Terms in Neural Network Backpropagation\n",
    "\n",
    "In the neural network code you shared, `delta_w_h_o` and `delta_w_i_h` represent the calculated adjustments that should be made to the network's weights during learning. Let me explain what each of these terms means and how they function in the training process.\n",
    "\n",
    "##### What is delta_w_h_o?\n",
    "\n",
    "`delta_w_h_o` stands for \"delta weights hidden to output.\" This variable represents the changes that should be applied to the weights connecting the hidden layer to the output layer.\n",
    "\n",
    "In the code, it's calculated as:\n",
    "```python\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_out\n",
    "```\n",
    "\n",
    "Breaking this down:\n",
    "- `learnrate` (0.5 in your example) controls how big the weight updates should be. A larger learning rate means larger steps in the weight space.\n",
    "- `output_error_term` represents how much the output was wrong, scaled by the derivative of the activation function. This tells us which direction to move in the weight space.\n",
    "- `hidden_layer_out` represents the activations of the hidden layer neurons. This ensures that connections from more active neurons receive proportionally larger updates.\n",
    "\n",
    "The shape of `delta_w_h_o` matches the shape of `weights_hidden_to_output`. In your example, `hidden_layer_out` is a vector with 2 elements (because you have 2 hidden neurons), so `delta_w_h_o` will also be a vector with 2 elements.\n",
    "\n",
    "Each element tells you how much to adjust the corresponding weight from a hidden neuron to the output neuron.\n",
    "\n",
    "##### What is delta_w_i_h?\n",
    "\n",
    "`delta_w_i_h` stands for \"delta weights input to hidden.\" This variable represents the changes that should be applied to the weights connecting the input layer to the hidden layer.\n",
    "\n",
    "In the code, it's calculated as:\n",
    "```python\n",
    "delta_w_i_h = learnrate * np.outer(x, hidden_error_term)\n",
    "```\n",
    "\n",
    "This calculation is more complex because it involves connections between multiple input neurons and multiple hidden neurons. Breaking it down:\n",
    "- `learnrate` again controls the size of the updates.\n",
    "- `x` is the input vector (with values [0.5, 0.1, -0.2] in your example).\n",
    "- `hidden_error_term` represents how much each hidden neuron contributed to the output error, scaled by the derivative of its activation function.\n",
    "- `np.outer()` creates a matrix where each element (i,j) is the product of the ith element of the first array and the jth element of the second array.\n",
    "\n",
    "The shape of `delta_w_i_h` matches the shape of `weights_input_to_hidden`. In your example, this will be a 3×2 matrix because you have 3 input features and 2 hidden neurons.\n",
    "\n",
    "Each element (i,j) in this matrix tells you how much to adjust the weight connecting input neuron i to hidden neuron j.\n",
    "\n",
    "##### How These Deltas Are Used\n",
    "\n",
    "After calculating these delta terms, the actual weight update would typically happen with:\n",
    "```python\n",
    "weights_hidden_to_output += delta_w_h_o\n",
    "weights_input_to_hidden += delta_w_i_h\n",
    "```\n",
    "\n",
    "This is the core of how neural networks learn. By iteratively:\n",
    "1. Making predictions (forward pass)\n",
    "2. Calculating errors\n",
    "3. Computing weight adjustments (backward pass)\n",
    "4. Updating weights\n",
    "\n",
    "The network gradually improves its predictions by adjusting its internal parameters.\n",
    "\n",
    "##### Concrete Example\n",
    "\n",
    "Let's say after running the code, we get:\n",
    "- `delta_w_h_o = [0.01, -0.02]`\n",
    "- `delta_w_i_h = [[0.005, -0.008], [0.001, -0.0016], [-0.002, 0.0032]]`\n",
    "\n",
    "This would mean:\n",
    "- The weight connecting the first hidden neuron to the output should increase by 0.01\n",
    "- The weight connecting the second hidden neuron to the output should decrease by 0.02\n",
    "- The weights connecting inputs to hidden neurons should change according to the `delta_w_i_h` matrix\n",
    "\n",
    "These small adjustments, repeated over many training examples, enable the network to gradually learn patterns in the data and make increasingly accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of the Calculated Weight Adjustments\n",
    "\n",
    "When you ran the backpropagation code, it produced these weight adjustment values:\n",
    "\n",
    "For hidden layer to output layer:\n",
    "```\n",
    "delta_w_h_o = [0.00804047, 0.00555918]\n",
    "```\n",
    "\n",
    "For input layer to hidden layer:\n",
    "```\n",
    "delta_w_i_h = [[ 1.77005547e-04, -5.11178506e-04]\n",
    "               [ 3.54011093e-05, -1.02235701e-04]\n",
    "               [-7.08022187e-05,  2.04471402e-04]]\n",
    "```\n",
    "\n",
    "Let me explain what these specific values tell us about the learning process in your network:\n",
    "\n",
    "##### Hidden to Output Layer Adjustments\n",
    "\n",
    "The `delta_w_h_o` values [0.00804047, 0.00555918] indicate that:\n",
    "\n",
    "1. The weight connecting the first hidden neuron to the output neuron should increase by approximately 0.008.\n",
    "2. The weight connecting the second hidden neuron to the output neuron should increase by approximately 0.0056.\n",
    "\n",
    "Both adjustments are positive, suggesting that strengthening these connections will help reduce the prediction error. The first hidden neuron's connection requires a slightly larger adjustment than the second, indicating it has more influence on correcting the current error.\n",
    "\n",
    "##### Input to Hidden Layer Adjustments\n",
    "\n",
    "The `delta_w_i_h` matrix shows smaller adjustments, mostly in the order of 10^-4 or 10^-5:\n",
    "\n",
    "```\n",
    "[[ 1.77005547e-04, -5.11178506e-04]\n",
    " [ 3.54011093e-05, -1.02235701e-04]\n",
    " [-7.08022187e-05,  2.04471402e-04]]\n",
    "```\n",
    "\n",
    "Reading this matrix row by row:\n",
    "\n",
    "1. For the first input neuron (with value 0.5):\n",
    "   - Its connection to the first hidden neuron should increase by 0.000177\n",
    "   - Its connection to the second hidden neuron should decrease by 0.000511\n",
    "\n",
    "2. For the second input neuron (with value 0.1):\n",
    "   - Its connection to the first hidden neuron should increase by 0.0000354\n",
    "   - Its connection to the second hidden neuron should decrease by 0.000102\n",
    "\n",
    "3. For the third input neuron (with value -0.2):\n",
    "   - Its connection to the first hidden neuron should decrease by 0.0000708\n",
    "   - Its connection to the second hidden neuron should increase by 0.000204\n",
    "\n",
    "The pattern of positive and negative adjustments reveals the complex interplay between inputs and hidden layer activations in correcting the network's prediction.\n",
    "\n",
    "##### Interpretation of the Values\n",
    "\n",
    "These relatively small weight adjustments are typical in neural network training. A few observations:\n",
    "\n",
    "1. The hidden-to-output weight adjustments are larger than the input-to-hidden adjustments. This is common in backpropagation, where gradients often diminish as they propagate backward (known as the \"vanishing gradient problem\" in deeper networks).\n",
    "\n",
    "2. The signs of the adjustments (positive or negative) indicate the direction needed to reduce error. Positive adjustments strengthen connections, while negative ones weaken them.\n",
    "\n",
    "3. The magnitudes reveal which connections are most important to adjust. For example, the largest adjustment is 0.00804047 for the first hidden-to-output weight.\n",
    "\n",
    "If you were to apply these adjustments to the original weights:\n",
    "\n",
    "```python\n",
    "# Original weights\n",
    "weights_hidden_to_output = np.array([0.1, -0.3])\n",
    "weights_input_to_hidden = np.array([\n",
    "    [0.5, -0.6],\n",
    "    [0.1, -0.2],\n",
    "    [0.1, 0.7]\n",
    "])\n",
    "\n",
    "# Updated weights\n",
    "weights_hidden_to_output += delta_w_h_o  # [0.10804047, -0.29444082]\n",
    "weights_input_to_hidden += delta_w_i_h   # Small adjustments to each weight\n",
    "```\n",
    "\n",
    "After many iterations of this process across multiple training examples, these small adjustments would accumulate and help the network converge toward weights that minimize prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the Backward Pass Implementation:\n",
    "\n",
    "1. **Calculate output error**: I computed the error as the difference between the target value and the actual output from our network.\n",
    "\n",
    "2. **Calculate error term for output layer**: For a sigmoid activation function, the error term is calculated as error * output * (1 - output). This formula comes from the derivative of the sigmoid function combined with the chain rule from calculus.\n",
    "\n",
    "3. **Calculate error term for hidden layer**: This is a two-step process:\n",
    "   - First, I propagated the error backward from the output layer to the hidden layer by multiplying the output error term with the weights connecting the hidden layer to the output layer.\n",
    "   - Then, I applied the sigmoid derivative to get the hidden layer error term.\n",
    "\n",
    "4. **Calculate weight changes for hidden-to-output layer**: The weight changes are computed as the learning rate multiplied by the output error term and the hidden layer outputs.\n",
    "\n",
    "5. **Calculate weight changes for input-to-hidden layer**: Here, I used the outer product of the input values and the hidden error terms, scaled by the learning rate. This gives us the appropriate weight update matrix with the same dimensions as the original weight matrix.\n",
    "\n",
    "This implementation follows the standard backpropagation algorithm, which adjusts weights based on the calculated error to minimize the difference between predicted and actual outputs."
   ]
  }
 ],
 "metadata": {
  "grader_mode": "",
  "kernelspec": {
   "display_name": "aipnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "showGradeBtn": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
